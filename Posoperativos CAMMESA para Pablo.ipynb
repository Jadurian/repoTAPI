{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import pyodbc\n",
    "import requests\n",
    "import io\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conexión a Base de Datos y formateo de fechas \"Desde\" y \"Hasta\" para bajar la data de CAMMESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lectura de archivo excel (Reemplazado por base de datos SQL en el futuro)\n",
    "archivo_excel = r'df_POSOP.xlsx'\n",
    "\n",
    "df = pd.read_excel(archivo_excel)\n",
    "\n",
    "# Converte la columna FECHA al formato datetime si no está en ese formato\n",
    "df['FECHA'] = pd.to_datetime(df['FECHA'])\n",
    "\n",
    "# Encuentra la última fecha y la convertierte al formato YYMMDD para servir como referencia al próximo archivo a descargar\n",
    "ultima_fecha = df['FECHA'].max()\n",
    "#Le suma un día para ir a buscar a CAMMESA\n",
    "Fecha_desde= ultima_fecha + timedelta(days=1)\n",
    "\n",
    "#Formateo Fecha para consutla de CAMMESA y para armado del nombre para extraer zip\n",
    "Fecha_desde_n = Fecha_desde.strftime('%y%m%d')\n",
    "Fecha_desde = Fecha_desde.strftime('%Y-%m-%dT%H:%M:%S.000-03:00')\n",
    "\n",
    "\n",
    "# Obtiene la fecha actual\n",
    "Fecha_hasta = datetime.now()\n",
    "\n",
    "#Formateo Fecha para consutla de CAMMESA y para armado del nombre para extraer zip\n",
    "Fecha_hasta_n = Fecha_hasta.strftime('%y%m%d')\n",
    "Fecha_hasta = Fecha_hasta.strftime('%Y-%m-%dT%H:%M:%S.000-03:00')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-02-07T16:37:41.000-03:00'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fecha_hasta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta de Info a CAMMESA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Fecha_desde' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m nemo \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPARTE_POST_OPERATIVO\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#Busco los zip disponibles para traer así puedo extraer el id\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m URL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.cammesa.com/pub-svc/public/findDocumentosByNemoRango?fechadesde=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mFecha_desde\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&fechahasta=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFecha_hasta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m&nemo=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnemo\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(URL)\n\u001b[0;32m      8\u001b[0m PPO\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Fecha_desde' is not defined"
     ]
    }
   ],
   "source": [
    "#URL para capturar Id del documento y el zip file:\n",
    "\n",
    "#Defino la tabla de CAMMESA que me voy a traer\n",
    "nemo = \"PARTE_POST_OPERATIVO\"\n",
    "#Busco los zip disponibles para traer así puedo extraer el id\n",
    "URL = f\"https://api.cammesa.com/pub-svc/public/findDocumentosByNemoRango?fechadesde={Fecha_desde}&fechahasta={Fecha_hasta}&nemo={nemo}\"\n",
    "response = requests.get(URL)\n",
    "PPO=response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_zip_file_url = \"https://api.cammesa.com/pub-svc/public/findAllAttachmentZipByNemoId?docId={}&nemo=PARTE_POST_OPERATIVO\"\n",
    "\n",
    "for i in range(len(PPO)):\n",
    "    # Construir la URL con el ID actual\n",
    "    zip_file_url = base_zip_file_url.format(PPO[i]['id'])\n",
    "    \n",
    "    # Realizar la solicitud GET para obtener el contenido del archivo ZIP\n",
    "    r = requests.get(zip_file_url)\n",
    "    \n",
    "    # Crear un objeto ZipFile a partir del contenido descargado\n",
    "    z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "    \n",
    "    # Directorio de destino para extraer los archivos ZIP\n",
    "    destination_directory = \"C:\\\\Users\\\\cvillena\\\\Downloads\"\n",
    "    \n",
    "    # Extraer todos los archivos del ZIP en el directorio específico\n",
    "    z.extractall(destination_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracción de datos a un dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genero función que me de los nombres de los zip en el formato deseado\n",
    "\n",
    "def generar_nombres_archivos(desde, hasta):\n",
    "    nombres_archivos = []\n",
    "    for year in range(int(desde[:2]), int(hasta[:2]) + 1):\n",
    "        for month in range(int(desde[2:4]), int(hasta[2:4]) + 1):\n",
    "            for day in range(int(desde[4:]), int(hasta[4:]) + 1):\n",
    "                nombre_archivo = f\"PO{year:02d}{month:02d}{day:02d}.zip\"\n",
    "                nombres_archivos.append(nombre_archivo)\n",
    "    return nombres_archivos\n",
    "\n",
    "\n",
    "\n",
    "# Generar los nombres de los archivos ZIP mensuales dentro del rango especificado\n",
    "archivos_zip = generar_nombres_archivos(Fecha_desde_n, Fecha_hasta_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO240204.zip\n",
      "PO240205.zip\n",
      "PO240206.zip\n",
      "El archivo PO240206.zip no se encontró. Saltando al siguiente archivo...\n",
      "PO240207.zip\n",
      "El archivo PO240207.zip no se encontró. Saltando al siguiente archivo...\n"
     ]
    }
   ],
   "source": [
    "dfs=[]\n",
    "for archivo_zip in archivos_zip:\n",
    "    try:\n",
    "        # Ruta completa del archivo ZIP mensual\n",
    "        ruta_completa = os.path.join(destination_directory, archivo_zip)\n",
    "        print(archivo_zip)\n",
    "        \n",
    "        # Extrae el archivo MDB de cada archivo ZIP mensual\n",
    "        with zipfile.ZipFile(ruta_completa, 'r') as zip_ref:\n",
    "            # Encontrar el nombre del archivo MDB dentro del ZIP mensual\n",
    "            archivo_mdb = os.path.splitext(archivo_zip)[0] + \".mdb\"\n",
    "            zip_ref.extract(archivo_mdb, path=destination_directory)\n",
    "\n",
    "        # Lee el archivo MDB y cargar la tabla VALORES_GENERADORES en un dataframe\n",
    "        mdb_file = os.path.join(destination_directory, archivo_mdb)\n",
    "        conn_str = f\"Driver={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={mdb_file};\"\n",
    "        conn = pyodbc.connect(conn_str)\n",
    "        df = pd.read_sql(\"SELECT * FROM VALORES_GENERADORES\", conn)\n",
    "        conn.close()\n",
    "\n",
    "        # Agregar el dataframe a la lista\n",
    "        dfs.append(df)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"El archivo {archivo_zip} no se encontró. Saltando al siguiente archivo...\")\n",
    "        continue\n",
    "\n",
    "# Concatenar verticalmente todos los dataframes mensuales en uno solo\n",
    "df_final = pd.concat(dfs, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subida de dataframe a base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por limitaciones de tamaño de excel filtramos solo las máquinas Pampa\n",
    "valores_filtrados = [\"ADTOHI\", \"AR21EO\", \"BAHIEO\", \"BBLATV29\", \"BBLATV30\",\n",
    "                      \"BBLMDI01\", \"BBLMDI02\", \"BBLMDI03\", \"BBLMDI04\", \n",
    "                      \"BBLMDI05\", \"BBLMDI06\", \"CERITV01\", \"CORTEO\", \n",
    "                      \"EBARTG01\", \"EBARTG02\", \"EBARTV01\", \"ETIGHI\", \n",
    "                      \"GEBATG01\", \"GEBATG02\", \"GEBATG03\", \"GEBATG04\", \n",
    "                      \"GEBATV01\", \"GEBATV02\", \"GUEMTG01\", \"GUEMTV11\", \n",
    "                      \"GUEMTV12\", \"GUEMTV13\", \"LDLATG01\", \"LDLATG02\", \n",
    "                      \"LDLATG03\", \"LDLATG04\", \"LDLATG05\", \"LDLATV01\", \n",
    "                      \"LDLMDI01\", \"LREYHB\", \"NIH1HI\", \"NIH2HI\", \"NIH3HI\", \n",
    "                      \"PAMEEO\", \"PEP3EO\", \"PILBDI01\", \"PILBDI02\", \n",
    "                      \"PILBDI03\", \"PILBDI04\", \"PILBDI05\", \"PILBDI06\", \"PIQIDI01\", \"PPLEHI\"]\n",
    "\n",
    "# Filtrar el DataFrame por los valores especificados en la columna \"GRUPO\"\n",
    "df_filtrado = df_final[df_final['GRUPO'].isin(valores_filtrados)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concatenado = pd.concat([df_final, df], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame exportado a Excel con éxito: C:\\Users\\cvillena\\Downloads\\df_POSOP.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Escribir el DataFrame final en un archivo Excel\n",
    "ruta_excel = \"C:\\Users\\jadurian\\OneDrive - Pampa Energia\\Laburo-Estudio\\Proyectos\\Tablero de Análisis de Pérdidas de Ingresos\\VSCode\\Data\\df_POSOP.xlsx\"\n",
    "df_concatenado.to_excel(ruta_excel, index=False)\n",
    "print(\"DataFrame exportado a Excel con éxito:\", ruta_excel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de la conexión a SQL Server\n",
    "path_zip = \".zips/PO240115.zip\"\n",
    "server = 'tu_servidor'\n",
    "database = 'tu_base_de_datos'\n",
    "username = 'tu_usuario'\n",
    "password = 'tu_contraseña'\n",
    "driver = '{ODBC Driver 17 for SQL Server}'\n",
    "\n",
    "# Función para conectar y leer la tabla específica del archivo .mdb\n",
    "def leer_tabla_mdb(nombre_tabla):\n",
    "    # Conectar al archivo .mdb dentro del .zip\n",
    "    with zipfile.ZipFile('archivo.zip') as zip_ref:\n",
    "        with zip_ref.open('archivo.mdb') as mdb_file:\n",
    "            conn_str = (\n",
    "                r'DRIVER={Microsoft Access Driver (*.mdb, *.accdb)};'\n",
    "                r'DBQ=path_to_your_mdb_file'\n",
    "            )\n",
    "            \n",
    "            conn = pyodbc.connect(conn_str)\n",
    "            cursor = conn.cursor()\n",
    "            \n",
    "            # Leer datos de la tabla especificada\n",
    "            cursor.execute(f'SELECT * FROM {nombre_tabla}')\n",
    "            \n",
    "            # Obtener todos los datos de la tabla\n",
    "            datos_tabla = cursor.fetchall()\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            return datos_tabla\n",
    "            pass\n",
    "\n",
    "# Función para insertar los datos en SQL Server\n",
    "def insertar_en_sql_server(datos, nombre_tabla_sql):\n",
    "    # Código para conectar a la base de datos SQL Server con pyodbc\n",
    "    conn = pyodbc.connect(f'DRIVER={driver};SERVER={server};DATABASE={database};UID={username};PWD={password}')\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    # Crear una tabla si no existe\n",
    "    cursor.execute(f\"IF NOT EXISTS (SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME = N'{nombre_tabla_sql}') CREATE TABLE {nombre_tabla_sql} (columna1 INT, columna2 VARCHAR(255))\")\n",
    "\n",
    "    # Insertar los datos en la tabla de SQL Server\n",
    "    for dato in datos:\n",
    "        cursor.execute(f\"INSERT INTO {nombre_tabla_sql} VALUES (?, ?)\", dato)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# Lógica principal\n",
    "nombre_tabla_mdb = 'nombre_tabla_mdb'\n",
    "nombre_tabla_sql = 'nombre_tabla_sql'\n",
    "\n",
    "datos_tabla_mdb = leer_tabla_mdb(nombre_tabla_mdb)\n",
    "\n",
    "if datos_tabla_mdb:\n",
    "    insertar_en_sql_server(datos_tabla_mdb, nombre_tabla_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPampa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
