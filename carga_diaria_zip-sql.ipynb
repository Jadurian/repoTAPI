{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import pyodbc\n",
    "import requests\n",
    "import io\n",
    "from datetime import datetime, timedelta\n",
    "import sqlalchemy\n",
    "import urllib\n",
    "\n",
    "from TAPI import capturar_fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: del .mdb genero un df y lo agrego a la tabla de la BBDD\n",
    "\n",
    "# DATOS DE LA BBDD, SERVER Y TABLA\n",
    "\n",
    "server = 'DARCCVWSQL19'\n",
    "#server = 'DESKTOP-37ESKFT\\SQLEXPRESS'\n",
    "database = 'TAPI'\n",
    "tabla = 'DiarioTest'\n",
    "\n",
    "# Fechas para seleccionar el día de la carga se debe iterar\n",
    "\n",
    "Fecha_desde = \"2024-03-13T00:00:00.000-03:00\"\n",
    "Fecha_hasta = \"2024-03-13T23:59:59.000-03:00\"\n",
    "\n",
    "#URL para capturar Id del documento y el zip file:\n",
    "\n",
    "#Defino la tabla de CAMMESA que me voy a traer\n",
    "NEMO = \"PARTE_POST_OPERATIVO_UNIF\"\n",
    "#Busco los zip disponibles para traer así puedo extraer el id\n",
    "URL = f\"https://api.cammesa.com/pub-svc/public/\"\n",
    "\n",
    "method_id = \"findDocumentosByNemoRango?\" #ID\n",
    "method_zip = \"findAllAttachmentZipByNemoId?\"\n",
    "zip_path = rf\"C:\\Users\\Jony\\OneDrive - Pampa Energia\\Laburo-Estudio\\Produccion\\Proyectos\\Tablero de Análisis de Pérdidas de Ingresos\\Proyecto\\.zips\"\n",
    "mdb_path = rf\"C:\\Users\\Jony\\OneDrive - Pampa Energia\\Laburo-Estudio\\Produccion\\Proyectos\\Tablero de Análisis de Pérdidas de Ingresos\\Proyecto\\.zips\\.mdb\"\n",
    "\n",
    "connection_string = f'DRIVER=ODBC Driver 17 for SQL Server;SERVER={server};DATABASE={database};Trusted_Connection=yes;'\n",
    "\n",
    "url_doc_id = f\"{URL}{method_id}fechadesde={Fecha_desde}&fechahasta={Fecha_hasta}&nemo={NEMO}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://api.cammesa.com/pub-svc/public/findDocumentosByNemoRango?fechadesde=2024-03-13T00:00:00.000-03:00&fechahasta=2024-03-13T23:59:59.000-03:00&nemo=PARTE_POST_OPERATIVO_UNIF'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url_doc_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 'F45F9A389262D01703258AE00077C432',\n",
       "  'fecha': '13/03/2024',\n",
       "  'nemo': 'PARTE_POST_OPERATIVO',\n",
       "  'titulo': 'Parte control post-operativo',\n",
       "  'comentario': '2',\n",
       "  'hora': '18:48',\n",
       "  'adjuntos': [{'id': 'PO240313.zip',\n",
       "    'campo': '$FILE',\n",
       "    'nombre': 'PO240313-(14/03/2024 18:48:10 ZW3).zip'}],\n",
       "  'version': '2024-03-14T18:48:00.000-03:00'}]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F45F9A389262D01703258AE00077C432\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    with requests.get(url_doc_id) as response:\n",
    "        if response.status_code == 200:\n",
    "            PPO=response.json()\n",
    "            display(PPO)\n",
    "            doc_id = PPO[-1]['id']\n",
    "            print(doc_id)\n",
    "        else:\n",
    "            print(\"La solicitud falló con el código de estado:\", response.status_code)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Manejar la excepción\n",
    "    print(\"Error al realizar la solicitud:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PO240313.zip\n"
     ]
    }
   ],
   "source": [
    "#conseguir el doc_id\n",
    "try:\n",
    "    with requests.get(url_doc_id) as response:\n",
    "        if response.status_code == 200:\n",
    "            PPO=response.json()\n",
    "            doc_id = PPO[-1]['id']\n",
    "            #print(doc_id)\n",
    "        else:\n",
    "            print(\"La solicitud falló con el código de estado:\", response.status_code)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Manejar la excepción\n",
    "    print(\"Error al realizar la solicitud:\", e)\n",
    "\n",
    "url_zip = f\"{URL}{method_zip}docId={doc_id}&nemo={NEMO}\"\n",
    "\n",
    "#descargar el .zip\n",
    "try:\n",
    "    with requests.get(url_doc_id) as response:\n",
    "        if response.status_code == 200:\n",
    "            r = requests.get(url_zip)\n",
    "\n",
    "            # Crear un objeto ZipFile a partir del contenido descargado\n",
    "            z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "\n",
    "            # Directorio de destino para extraer los archivos ZIP\n",
    "            destination_directory = \".zips\"\n",
    "\n",
    "            # Extraer todos los archivos del ZIP en el directorio específico\n",
    "            z.extractall(destination_directory)\n",
    "            zip_name = z.namelist()[0]\n",
    "\n",
    "            #dia_mdb = capturar_fecha(name_zip)\n",
    "            print(zip_name)\n",
    "        else:\n",
    "            print(\"La solicitud falló con el código de estado:\", response.status_code)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    # Manejar la excepción\n",
    "    print(\"Error al realizar la solicitud:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PO240313.zip'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.date(2024, 3, 13)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Función para capturar la fecha del nombre del archivo\n",
    "\n",
    "def fecha_dia(nombre_archivo):\n",
    "    fecha_str = nombre_archivo[2:4] + \"-\" + nombre_archivo[4:6] + \"-\" + nombre_archivo[6:8]\n",
    "    #print(fecha_str)\n",
    "    return datetime.strptime(fecha_str, '%y-%m-%d').date()\n",
    "\n",
    "\n",
    "dia_mdb = fecha_dia(zip_name)\n",
    "dia_mdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jadurian\\AppData\\Local\\Temp\\ipykernel_2692\\1455759821.py:17: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  valores_generadores = pd.read_sql(\"SELECT * FROM VALORES_GENERADORES\", conn)\n"
     ]
    }
   ],
   "source": [
    "#Colocar los PATHs correctos donde se traeran los archivos\n",
    "\n",
    "zip_path = rf\"C:\\Users\\Jadurian\\OneDrive - Pampa Energia\\Laburo-Estudio\\Produccion\\Proyectos\\Tablero de Análisis de Pérdidas de Ingresos\\Proyecto\\.zips\\{zip_name}\"\n",
    "mdb_path = rf\"C:\\Users\\Jadurian\\OneDrive - Pampa Energia\\Laburo-Estudio\\Produccion\\Proyectos\\Tablero de Análisis de Pérdidas de Ingresos\\Proyecto\\.zips\\.mdb\"\n",
    "try:\n",
    "  \n",
    "    # Extrae el archivo MDB de cada archivo ZIP mensual\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        # Encontrar el nombre del archivo MDB dentro del ZIP mensual\n",
    "        archivo_mdb = os.path.splitext(zip_name)[0] + \".mdb\"\n",
    "        zip_ref.extract(archivo_mdb, path=mdb_path)\n",
    "\n",
    "    # Lee el archivo MDB y cargar la tabla VALORES_GENERADORES en un dataframe\n",
    "    mdb_file = os.path.join(mdb_path, archivo_mdb)\n",
    "    conn_str = f\"Driver={{Microsoft Access Driver (*.mdb, *.accdb)}};DBQ={mdb_file};\"\n",
    "    conn = pyodbc.connect(conn_str)\n",
    "    valores_generadores = pd.read_sql(\"SELECT * FROM VALORES_GENERADORES\", conn)\n",
    "    conn.close()\n",
    "    valores_generadores.insert(0, 'FECHA', dia_mdb)\n",
    "\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"El archivo {zip_name} no se encontró. Saltando al siguiente archivo...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15936, 40)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valores_generadores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: Cargamos el Dataframe a la tabla de sql con sqlalchemy\n",
    "\n",
    "quoted = urllib.parse.quote_plus(connection_string)\n",
    "\n",
    "engine = sqlalchemy.create_engine('mssql+pyodbc:///?odbc_connect={}'.format(quoted))\n",
    "\n",
    "valores_generadores.to_sql('DiarioTest', schema='dbo', con=engine, if_exists='append')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15864, 40)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valores_generadores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "('The SQL contains 39 parameter markers, but 40 parameters were supplied', 'HY000')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mINSERT INTO DiarioTest (FECHA, GRUPO, HORA, ARL, ESTADO_OPE, PI_SOTR, ARL_ECON, EST_REMUN, ENERGIA, PRECIO_NODO, PR_REM_ENERGIA, SMEC, POT_DISP, POT_OPE_CPO, COMPRA_SPOT, SCTD, SCO, PIND, PINDPROG, PINDFORZ, POT_DISP_GAS, POT_DISP_RESERVA, GAS_NOMINADO, COSTO_406, REM_ADICIONAL, DESP_ECON, CCM, CFO, CGN, CGO, POT_DISP_R19, FACTOR_KM, HORA_REPRESENTATIVA, ENERG_GEN_TRANSAC, ENERG_OPE_TRANSAC, SOBREC_FORZ_PROPIO, PGENE_RES354, CGU_PROPIO, CGU_MEM, CGU_GAS_CEDIDO) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Ejecutar la consulta SQL con los valores de la fila actual\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Confirmar los cambios\u001b[39;00m\n\u001b[0;32m     15\u001b[0m conneccion\u001b[38;5;241m.\u001b[39mcommit()\n",
      "\u001b[1;31mProgrammingError\u001b[0m: ('The SQL contains 39 parameter markers, but 40 parameters were supplied', 'HY000')"
     ]
    }
   ],
   "source": [
    "#TODO: generar una tabla que sea la concatenación de todas las tablas de 'PMBD Component Degradation'\"Prot_v2_TEST copy.ipynb\"\n",
    "contador = 0\n",
    "\n",
    "conneccion = pyodbc.connect(connection_string)\n",
    "\n",
    "cursor = conneccion.cursor()\n",
    "\n",
    "for index, row in valores_generadores.iterrows():\n",
    "    # Crear la consulta SQL para insertar la fila en la tabla\n",
    "    query = f\"INSERT INTO DiarioTest (FECHA, GRUPO, HORA, ARL, ESTADO_OPE, PI_SOTR, ARL_ECON, EST_REMUN, ENERGIA, PRECIO_NODO, PR_REM_ENERGIA, SMEC, POT_DISP, POT_OPE_CPO, COMPRA_SPOT, SCTD, SCO, PIND, PINDPROG, PINDFORZ, POT_DISP_GAS, POT_DISP_RESERVA, GAS_NOMINADO, COSTO_406, REM_ADICIONAL, DESP_ECON, CCM, CFO, CGN, CGO, POT_DISP_R19, FACTOR_KM, HORA_REPRESENTATIVA, ENERG_GEN_TRANSAC, ENERG_OPE_TRANSAC, SOBREC_FORZ_PROPIO, PGENE_RES354, CGU_PROPIO, CGU_MEM, CGU_GAS_CEDIDO) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\"\n",
    "    \n",
    "    # Ejecutar la consulta SQL con los valores de la fila actual\n",
    "    cursor.execute(query, tuple(row))\n",
    "    # Confirmar los cambios\n",
    "    conneccion.commit()\n",
    "\n",
    "# Cerrar el cursor y la conexión\n",
    "cursor.close()\n",
    "conneccion.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvPC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
